# -*- coding: utf-8 -*-
"""Sarcasm_Detector_DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11rL-xAXyVQMkzOR2XkngO2BMlL6wYZS9
"""

#importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re                            #Helps to remove unnecessary symbols that do not provide any meaning such as @, comma, hashtags, *,etc.
import string
from nltk.corpus import stopwords    #removes stop words, lemmatize the sentences, and many different functions can be performed using NLTK.
from wordcloud import WordCloud

#importing libraries for model building and evaluation
from tensorflow.keras.preprocessing.text import Tokenizer      #Used to split text into individual units, such as words or sub words which make it easier to analyze and process text for various language-related tasks.
from tensorflow.keras.preprocessing.sequence import pad_sequences

from sklearn.metrics import classification_report, confusion_matrix

"""# REPRESENTATION OF DATASET"""

import json
import pandas as pd

data = []
with open('/content/Sarcasm_Headlines_Dataset.json', 'r') as file:
    for line in file:
        try:
            data.append(json.loads(line))
        except json.JSONDecodeError:
            continue  # Skip lines that cannot be parsed

df = pd.DataFrame(data)
print(df.head())

df.info()

# Check the dataset label balance or not

# Check number of headlines by is_sarcastics
plt.figure(figsize=(10, 5))
sns.countplot(x='is_sarcastic', data=df, palette="summer").set_title(
	"Countplot of headlines")
plt.show()

#downloading the stopwords corpus list
import nltk


nltk.download('stopwords')
stopwords_list = stopwords.words('english')

def clean_text(sentences):
	# convert text to lowercase
	text = sentences.lower()
	# remove text in square brackets
	text = re.sub('\[.*?\]', '', text)
	# removing punctuations
	text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
	# removing words containing digits
	text = re.sub('\w*\d\w*', '', text)
	# Join the words
	text = ' '.join([word for word in text.split()
					if word not in stopwords_list])
	return text


print(df['headline'].iloc[1])
clean_text(df['headline'].iloc[1])

#new column to store cleaned text
df['cleaned_headline']=df['headline'].map(clean_text)

# Combine all sarcastic cleaned headlines into a single text
import matplotlib.pyplot as plt
from wordcloud import WordCloud
Sarcastic_text = ' '.join(
	df['cleaned_headline'][df['is_sarcastic'] == 1].tolist())

# Import the necessary libraries

# Create a WordCloud object with specified width, height, and background color
wordcloud = WordCloud(width=800, height=400,
					background_color='black').generate(Sarcastic_text)

# Display the WordCloud without axes
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Sarcastic')
plt.show()

# Combine all non-sarcastic cleaned headlines into a single text
Non_Sarcastic_text = ' '.join(
	df['cleaned_headline'][df['is_sarcastic'] == 0].tolist())

# Create a WordCloud object with specified width, height, and background color
wordcloud = WordCloud(width=800, height=400,
					background_color='black').generate(Non_Sarcastic_text)

# Display the WordCloud without axes
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Not Sarcastic')
plt.show()

"""# **TRAINING OF THE MODEL**"""

#coverting the columns into lists
text = df['cleaned_headline'].tolist()
label = df['is_sarcastic'].tolist()

# train :test : validation = 80:10:10
train_portion = .8

# Set the train size using training_portion arg
train_size = int(len(text) * train_portion)

# Training dataset
train_text = text[:train_size]
train_label = label[:train_size]
# Validations dataset
valid_size = train_size+int((len(text)-train_size)/2)
val_text = text[train_size:valid_size]
val_label = label[train_size:valid_size]
# Testing dataset
test_text = text[valid_size:]
test_label = label[valid_size:]

# Check
print('Training data :', len(train_text), len(train_label))
print('Validations data :', len(val_text), len(val_label))
print('Testing data :', len(test_text), len(test_label))

# Set parameters
# Max len of unique words
vocab_size = 10000

# Embedding dimension value
embedding_dim = 200

# Max length of sentence
max_length = 60

# pad_sequences arg
padding_type = 'post'

# Unknow words = <OOV>
oov_tok = '<OOV>'

# Tokenizing and padding
# Create a tokenizer with a specified vocabulary size and out-of-vocabulary token
tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
# Fit the tokenizer on the training text data to create word-to-index mapping
tokenizer.fit_on_texts(train_text)

# Get the word index from the tokenizer
word_index = tokenizer.word_index

#Printing the word_index
word_index

# Convert training text to sequences of word indices
tokenizer.texts_to_sequences(train_text[:5])

# Tokenize and pad the training text data
# Convert training text to sequences of word indices
train_indices = tokenizer.texts_to_sequences(train_text)
# Pad sequences to a fixed length
train_padded = pad_sequences(train_indices,
							padding=padding_type,
							maxlen=max_length)

# Convert validation text to sequences of word indices
val_indices = tokenizer.texts_to_sequences(val_text)
# Pad sequences to a fixed length
validation_padded = pad_sequences(val_indices,
								padding=padding_type,
								maxlen=max_length)

# Convert test text to sequences of word indices
test_indices = tokenizer.texts_to_sequences(test_text)
# Pad sequences to a fixed length
test_padded = pad_sequences(test_indices,
							padding=padding_type,
							maxlen=max_length)

# Check
print('Training vector :', train_padded.shape)
print('Validations vector :', validation_padded.shape)
print('Testing vector :', test_padded.shape)

# Decode the sample training vector
tokenizer.sequences_to_texts([train_padded[0].tolist()])

# Prepare labels for model
training_labels_final = np.array(train_label)
validation_labels_final = np.array(val_label)
testing_labels_final = np.array(test_label)


# Check shapes
print('Training vector :', training_labels_final.shape)
print('Validations vector :', validation_labels_final.shape)
print('Testing vector :', testing_labels_final.shape)

# Import the TensorFlow library
import tensorflow as tf

# Define a sequential neural network model
model = tf.keras.Sequential([
	# Embedding layer for creating word embeddings
	tf.keras.layers.Embedding(
		vocab_size, embedding_dim, input_length=max_length),

	# GlobalMaxPooling layer to extract relevant features
	tf.keras.layers.GlobalMaxPool1D(),

	# First Dense layer with 40 neurons and ReLU activation
	tf.keras.layers.Dense(40, activation='relu'),

	# Dropout layer to prevent overfitting
	tf.keras.layers.Dropout(0.5),

	# Second Dense layer with 20 neurons and ReLU activation
	tf.keras.layers.Dense(20, activation='relu'),

	# Dropout layer to prevent overfitting
	tf.keras.layers.Dropout(0.5),

	# Third Dense layer with 10 neurons and ReLU activation
	tf.keras.layers.Dense(10, activation='relu'),

	# Dropout layer to prevent overfitting
	tf.keras.layers.Dropout(0.2),

	# Final Dense layer with 1 neuron and sigmoid activation for binary classification
	tf.keras.layers.Dense(1, activation='sigmoid')
])

model.summary()

# Compile the model with specified loss function, optimizer, and evaluation metrics
model.compile(loss='binary_crossentropy',
			optimizer='adam', metrics=['accuracy'])

# Set the number of training epochs
num_epochs = 5

# Fit the model to the training data and validate on the validation data
history = model.fit(
	train_padded, training_labels_final,
	epochs=num_epochs,
	validation_data=(validation_padded, validation_labels_final)
)

"""# LOSS AND ACCURACY"""

# Create a figure with subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot validation loss
ax1.plot(history.history['loss'], label='Training Loss')
ax1.plot(history.history['val_loss'], label='Validation Loss',color='orange')
ax1.set_title('Validation Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend()

# Plot validation accuracy
ax2.plot(history.history['accuracy'], label='Training Accuracy')
ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
ax2.set_title('Validation Accuracy')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy')
ax2.legend()

# Adjust layout
plt.tight_layout()

# Show the plots
plt.show()

#evaluating loss and accuray of the model
loss, accuracy = model.evaluate(test_padded,testing_labels_final)
print(f'Accurcy on test dataset :{round(accuracy*100,2)}%')

#predicting the model

pred_prob = model.predict(test_padded)
pred_label = [1 if prob >= 0.5 else 0 for prob in pred_prob]
pred_label[:5]

# Compute confusion matrix using the predicted classes and true labels
conf_matrix = confusion_matrix(testing_labels_final, pred_label)

# Plot the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
			xticklabels=['Not Sarcastic', 'Sarcastic'],
			yticklabels=['Not Sarcastic', 'Sarcastic'])
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Print Classification Report
print("\nClassification Report:")
print(classification_report(testing_labels_final, pred_label,
							target_names=['Not Sarcastic', 'Sarcastic']))

while True:
	# Prompt the user to input a headline or exit
	user_input = input(
		"Enter a headline for prediction (or type 'exit' to quit): ")

	# Check if the user wants to exit
	if user_input.lower() == 'exit':
		break

	# Clean and preprocess user input
	cleaned_input = clean_text(user_input)
	tokenized_input = tokenizer.texts_to_sequences(
		[cleaned_input]) # Tokenize the cleaned text
	padded_input = pad_sequences(
		tokenized_input, maxlen=max_length, padding=padding_type) # Pad the tokenized text

	# Predict sarcasm
	prediction = model.predict(padded_input)

	# Print the prediction result
	if prediction >= 0.5:
		print(f"Headline: {user_input}\nPrediction: Sarcastic")
	else:
		print(f"Headline: {user_input}\nPrediction: Not Sarcastic")

